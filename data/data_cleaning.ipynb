{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint 2: Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that I have chosen covers traffic accidents in the United States from 2016-2023. The data was collected from various government sources, such as state and federal departments of transportation, and compiled into one dataset. I chose this dataset because of its size and robust feature set, which should allow for many interesting visualizations and analysis.\n",
    "\n",
    "The current version of the datset is from Kaggle: https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents?resource=download\n",
    "\n",
    "Below are citations for the papers for the original creation of the dataset:\n",
    "\n",
    "Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. “A Countrywide Traffic Accident Dataset.”, 2019.\n",
    "\n",
    "Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. \"Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.\" In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from dash import Dash, dcc, html, Input, Output\n",
    "import addfips\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data from csv\n",
    "df_raw = pd.read_csv('US_Accidents_March23.csv')\n",
    "# display preview of dataset\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get basic info about dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables will be removed if they don't seem to provide any opportunity for interesting analysis, or if they seem redundant because there are already other, better variables than can be used for a certain aspect of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted or redundant variables\n",
    "df = df.drop(columns={'ID', 'Source', 'End_Lat', 'End_Lng', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Wind_Direction', 'Weather_Timestamp', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight', 'Turning_Loop'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove entries where some more important variables are null\n",
    "df.dropna(subset={'Description', 'Street', 'City', 'Sunrise_Sunset', 'Temperature(F)','Weather_Condition', 'Temperature(F)', 'Visibility(mi)' }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining variables, wind speed and precipitation, the null values will be filled with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The dataset now has no null values. A few more steps are needed to prepare the dataset for use with the dashboard. The first of these is to add FIPS codes for counties and states, which will be used to make it easier to plot the data on a choropleth."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "af = addfips.AddFIPS()\n",
    "\n",
    "df['StateFIPS'] = '0'\n",
    "df['CountyFIPS'] = '0'\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index % 10000 == 0:\n",
    "        print(index)\n",
    "    county = row['County']\n",
    "    state = row['State']\n",
    "    state_fips = af.get_state_fips(state)\n",
    "    county_fips = af.get_county_fips(county, state=state)\n",
    "    df.at[index, 'StateFIPS'] = state_fips\n",
    "    df.at[index, 'CountyFIPS'] = county_fips"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The next step is to add unix timestamps to each row, which will make it easier to filter data points by date."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_unix_timestamp(data_row):\n",
    "    date = pd.to_datetime(data_row['Start_Time'])\n",
    "    unix_timestamp  = int(time.mktime(date.timetuple()))\n",
    "    return unix_timestamp\n",
    "\n",
    "df['timestamp'] = df.apply(get_unix_timestamp, axis=1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, a few more columns can be dropped."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = df.drop(columns={'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng', 'Description'})"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, the dataset needs to be truncated for use with the dashboard, since the full dataset is larger than the RAM provided by Render. A sample of 400k data points will be used."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = df.sample(400000)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df.to_csv('clean_data.csv', index=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential UI and Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two good UI elements would be a drop down allowing selection of multiple numeric variables (like date), as well as sliders for selected numerical variables to select the range to be visualized. Another good UI element could be radio selection for categorical variables to visualize, like weather condition. One more could be checkboxes to select entries with certain boolean variables true, like whether a certain traffic feature was present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best visualization for this dataset is probably an interactive map. The map can be broken down into either states or counties, and it can be colored according to the frequency of accidents based on the filters selected by the UI elements above. Another possible visualization is a scatter plot of two differect selectable variables to see how they correlate. Another possible visualization is a pie chart showing what proportion of all total accidents have a certain trait, such as breakdown of accident frequency by time of day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds4003",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
